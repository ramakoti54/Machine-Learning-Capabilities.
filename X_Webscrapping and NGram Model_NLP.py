# Defining Modules used for Designing N-Gram model of LLM.
import tensorflow as tf;
import os;
from tensorflow import keras;
#import URLlib.request as urs;
import webbrowser as wb;
import re;
import urllib.request as urs

# Algorithmic program to illustrate Machine Learning algorithms usage on Webscrapping from X
# Step1: Forming a vector/array with the required token sizes for storing different words of an Web URL.
# Step2: Calling URLlib.request module to connect to the respective website X with an URL as "https://x.com/"
# Step3:

# 1. Time+Space complexity to use Vector vs Dictionary to understand usage of datatypes.
# 2. Using the same string for tf.tensor to represent data types.

class N_Model:
   def Web_scrapping(URL,Content):
      with urs.open(URL):
         response = urs.response(URL);
         Content = response.read();
         return str(Content);

   def Tokenization(Content):
      #for con in Content:
      #   if(re.split('.',Content,re.IGNORECASE)):
      X_Content = re.split('\. ',Content,re.IGNORECASE);
      k = len(X_Content);
      vector_tokens = [[0,0,0],[0,0,0],[0,0,0]];
      Sentences = "";
      Words = [];
      for tokens in  X_Content:
         #for tokens in X_Content:
         #token[tokens] = i;
         #i = i+1;
         #print(vector_tokens);
         #Sentences.append('tokens');
         Sentences = tokens;
         Split_Sentence = Sentences.split(" ");
         Words.append(Split_Sentence);
      return(Words);

   # Move NGram_Model outside of Tokenization
   def NGram_Model(Tokens,i,j,Token,Token_Values):
      for words in Tokens:
         for word in words:
            #print(word)
            if(i < len(words)):
               Token.append(word);
               Token_Values.append(i);
               #for i in word:
               #   print(word[i]);
               i = i+1;
            else:
               #print("Number of words are zero. Check the Webpage comments");
               continue;
      Token_Vector = [Token,Token_Values]
      return(Token_Vector);

URL = "https://x.com/";
#Token_Value = 1;
#Vector = [];
Content = "";
#Tokens = [];
#Vector_Size =

N_model_object = N_Model();
X_Content = N_model_object.Web_scrapping(URL,Content);
Tokens = Tokenization(X_Content);
i=0;
j=0;
Token_keys = [];
Token_Values = [];
Token_Vector = N_model_object.NGram_Model(Tokens,i,j,Token_keys,Token_Values);
print(Token_Vector[0],"\n",Token_Vector[1]);
