********  Feature Engineering Principles and Selection  *********

  Feature Engineering methods are multitude of mathematical operations (An array of techniques) helpful to apply on supervised, unsupervised, semi supervised and reinforcement ML models to optimize them in standard normalizing
  dataset prior to applying training and simulations. These features aid the ML model to raise the efficacy of the model by optimizing input data samples.

  Features Engineering principles help to develop various data transformations to extract data features. These principles are similar in nature if they belong to the same data class and dissimilar otherwise.

    The feature engineering principles are several types such as 
    1. Feature Selection methods  2. Histogram  3. Clustering  4. Principal component analysis
 (PCA)

  Histogram:
            Histogram is a group of features represented with buckets and each bucket is of size n and frequency m. These buckets are of either symetric or Asymmetric in nature.
Symmetric histograms consist of the size of each bucket remains same and frequency of each bucket varies on the vertical axle and depends on features present in the bucket. Histogram bins are useful in representation of textual, image, audio and categorical datasets.

Asymmetric histograms contain distinct size buckets in representing features of multiple principles. The following diagrams show the importance of both of these variations.

In the following diagram fig 1.0 represents the population of multiple regions include, Delhi, Bangalore, Hyderabad and Chennai. The bars in histogram are symmetrical in nature with equal width population frequency measures. All of the cities had  population numbers represented on the  Y-axis in Millions and city names with colour codes depicted on the X-axis.

		Fig 1.0  Illustration of Regional population(Couldn't show the images in figures here due to limitation of showing images on github content. Visit the shared link for access to understand about the Feature engineering principles).

URL for Drive access for requesting view access: 
                https://docs.google.com/document/d/1SAJlFKia_XzMZm6oB361M5D7mZvDGTOnkxRiY6DQrY0/edit?usp=sharing

Asymmetric nature of the histogram for the same example shown in the following diagram. The illustration of population sizes for both North and South region cities illustrated over the histogram bars with different width bars to represent city names and the separated based on the distinct width of each bar. The first bar to second bar had been a width of two and the next with three and the last separates from the previous one with two in size.  

Feature Selection:
		Human Interaction plays an important role in machine learning models to identify the components of the model and find logic to integrate them into a complex configuration for using them in Organizations. Feature selection is supervised-learning one of such approach helps to shrink size of dimensionality (Reduces from 3D to 2D) and removes unrelated features and keeps the prominent features to optimize models with low computations.

Principle component analysis (PCA) approach aids to reduce dimension of the input dataset to optimize features and training ML models in both Supervised and Un-supervised along with Semi supervised, Reinforcement Generative AI models.

Historical features of Categorical Data:
		Histogram represents both symmetric and Asymmetric types of data. The relative ML models deals with distinct data types includes text, categorical, image and Audio types.

Categorical dataset is a type which allows ML(applications) model to deal with Qualitative selection of the data. It Describes about features through Non-numeric representation. 
For Instance, consider the blood type of a sampled population in blood donation camp. Generally to represent Blood pressure, weight, blood levels use numerical values while communicating by medical professionals. These blood checks could help to determine the blood is usefulness for other patients in the hospital. 

However, blood type is represented with categorical data types with alphabets O, A, B and AB with both positive and negative to differentiate the polarization of blood. These represent normal assigning values and each categorical label is associated with a numeric value to understand about the relation among each of the types.

Blood type				Representation
========				============
O					      0
A					      1
B					      2
AB					    3

O			A			B			AB
 ._________________._________________.__________________.
|			|                                   			   |	
	|           close/similar	|                                   			   |
| —-------------------------- 		Distinct/Dissimilar		   |	
|------------------------------------------------------------------------------------------|

			Fig 3.0 Categorical Data types First representation.
  
O			A			AB			B
 ._________________._________________.__________________.
|			|                                   |			   	
	|           close/similar	|                                   |			   
| —-------------------------- Distinct/Dissimilar     |	
|---------------------------------------------------------- |

Fig 4.0 Categorical Data types with second representation


As mentioned in both Fig 3.0 and Fig 4.0 categorical data could be represented in multiple ways but the meaning of the data makes it complex to understand by the reader to create consistency of the dataset.

Hence, there is another precise way of representing categorical data through histogram based approach called one-hot encoding.

One-hot encoding is an approach consists of vectors and multitude of arrays(matrices) to represent one categorical value to be highlighted at a time by replacing other dimensional features with zero(0).

O	1	0	0	0
A	0	1	0	0
B	0	0	1	0
AB	0	0	0	1
	O	A	B	AB


Histogram Features of Text Data:
		Histogram features for textual data illustrated with a feature representation vector called as feature vector or Bag of Words(BOW).

BOW feature vector used to store textual linguistic words from sentences of textual dataset. Usually, BOW vector stores significantly impactful words of the text from the sentences to simplify the process of computations.

In Bow representation feature vectors assigned with respective frequency counts to represent each linguistic word.
For instance, consider two sentences to process for ML models are
The Dogs are best pets.
The Cats are worst pets.

Two of the sentences are formed in the maintenance of animals(pets) at home and consider BOW vectors as
Cats - 1/sqrt(3)			- 0

Dogs - 0				- 1/sqrt(3)

Best - 0				- 1/sqrt(3)

Worst - 1/sqrt(3)			- 0

Pets - 1/sqrt(3)			- 1/sqrt(3)
=========				=========	
  X(input)				XT(input)
=========				=========

The input X features are assigned with a BOW feature input value 1/sqrt(3) since three features are significant in first vector and distinct three words are important in the latter.

XT is the transpose vector used to represent the second sentence. Always the inner product of these vectors lies between 0 and 1. Hence the correlation among these features is identified based on the inner product relation. 

Considering pets as a significant words statements has a co-relation factor of 0.33 (1/sqrt(3)*1/sqrt(3)) to leave the cofactor value in less than half of the statement symmetry.
Bow vectors becomes symmetrical if we have statements as 
“some Dogs are smarter than cats and some Cats are smarter than Dogs”.

Dogs		-1/sqrt(2)	-0
Cats		-0		-1/sqrt(2)
Smarter	-1/sqrt(2)	-1/sqrt(2)

The above statement has a better co-relation factor value of 1. The adjective describing both of the animals as smarter in some situations would give the same correlation while considering one animal at the same time.

Bow representation is hugely used in realtime Machine Learning applications like spam detection NLP processing and LLM feature analysis techniques, categorization of emails, news articles and social media posts etc…

Example 1: Histogram features for Sentiment Analysis:(An example of Textual Data)
	Another interesting area for Histograms to illustrate the behaviour of feature methods is through performing analysis on the vast majority of subject areas includes banking, finance domains. The sentiment analysis is a classified method to demonstrate results on different domains through social network comments, providing feedback through forms to result into either positive, negative or neutral sentiment.

For example, consider 

“Cricket is the greatest sport of all times mentioned by the legend of cricket in 1930’s called Don Bradman. He is well known for his most number of centuries in fewer games.”
“Baseball is another sport that significantly impacts many sportsmen across northern America. It is played similar to Cricket.”

The two statements seem distinct in nature but they do have a co-relation while impacting the sportsmen life around the world. The mutual dependency between two scenarios are quite slim compared to the earlier example in textual data of Histogram features. The Classification of two statements depends on the label prediction requirements of these statements.


BOW for statement 1:

1/14 - 0

Cricket great sport time mention legend don bradman well know most centuries few games

BOW for statement 2:

0- 1/11

Baseball another sport significant impact sportsmen across Northern-america played similar cricket

While considering the co-relation among the two statements the number of bow features from the statement 1 shows as 14 and is considered as 11 in the second statement. The total probability for computing the BOW significance for the statement 1 is computed as 1/14 while other statements have no significance to represent them in eigen vectors. The same holds 1/11 for the statement 2 while considering the BOW eigen vector.

The dot product of these two feature vectors gives a coefficient value of (1/14 * 1/11 = 0.080) and the value stays between 0 and 1.

Based on understanding about these two statements they can be classified as least co related among the BOW feature examples.

The co-relation among these statements can be represented with a sine and cosine curve as shown below.



Example 2: Spam Detection (Example of Histogram feature for Textual Analysis.):
		Spam related web content has grown over the years and major industries use the feature engineering principle in the current world. The examples include google’s email chain with a spam detection process to identify and classify the email to be assigned to the spam folder, The user notification process in networking communication by carriers to send text messages to their customers with unlimited data/roaming data offers with non-existed links.
Generally, BOW identifies if the email is either spam or another class label. BOW lists the words like bargain, act now, urgent, time constraint etc.. could be used in forming softmax cost function.

Additionally, classification features include class features, char word count and spam detection features etc… improves the performance of machine learning spam detection methods.

The process can be illustrated through a diagram here.

The Hyperlink to access the diagrams and tables of the content shown below.

URL: https://docs.google.com/document/d/1o8e7nFryFLEafNwMDq7OcOJEKDP2pUcp/edit?usp=sharing&ouid=110429265379294196744&rtpof=true&sd=true




